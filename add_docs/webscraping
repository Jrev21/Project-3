WEB SCRAPING TUTORIAL
by Justin Yeary

REQUIREMENTS:
1) Ammonite scripting package - https://ammonite.io/#ScalaScripts
2) Jsoup web scraping for Scala - https://www.lihaoyi.com/post/ScrapingWebsitesusingScalaandJsoup.html
3) Scala properly configured in Linux terminal

NOTE: THe code I present to you here was the result of hours of experimentation, frustration, and messing around in the terminal. Sometimes this results in me making new variables, for exmaple mytags and mytags2. I jump around with variable numbers in my code because I printed my terminal output when all was said and done, and then parsed it and shared with you only the code that actually works. So if the variable names seem to jump around a bit, that's why. The overall logic works, and I have gone back and cleaned up the inputs.

OVERVIEW:
The overall goal here is to use Ammonite to run Scala as a scripting language to scrape the census bureau's FTP servers to download all 53 state/territory census files for 2020, 2010 and 2000. I will demonstrate how this is accomplished for the 2020 census bureau FTP server. There are slight alterations for 2010 and 2000, but overall the same strategy applies.

First, open up your Linux terminal and run Ammonite. Then import Jsoup library and other packages. Use the following commands:

~$ amm
import $ivy.$                       , org.jsoup._
import collection.JavaConverters._     #you will need this to convert between Lists and Array Buffers
import scala.language.postfixOps       #Not sure why this is needed but the scripts wouldn't run correctly without it
import java.net.URL                    #You need this to tell Scala to treat URL as unique object
import java.io.File                    #You need this to actually download the zip files from the website
import sys.process._                   


The first step is to turn the FTP server (https://www2.census.gov/programs-surveys/decennial/2020/data/01-Redistricting_File--PL_94-171/) into an HTML object in order to begin parsing it. We use the following code:

val doc = Jsoup.connect("https://www2.census.gov/programs-surveys/decennial/2020/data/01-Redistricting_File--PL_94-171/").get()

This command will return some text that looks exactly as you would expect the HTML tags of a website to look like. If we investigate the website, we see that the relevant links we want are contained in the a tags that are nested as follows: <tbody><tr><td><a>~<a>~<a>... (the ~ denote sibling relationship). Therefore our next set is to parse our document down to a collection of these a tags. You accomplish this as follows:

val states = doc.select("tbody tr td a").asScala

This returns an array buffer containing all of the a statements as object type nodes.Element. Pay attention to object types here. Scala is notoriously finnicky about type classes and you will need this later on in the code. THe next step is now to collect the text elements of the node.Element objects in our array buffer called states. We accomplish this with a for-comprehension (one of Scala's most powerful tools!) as follows:

val stateNames = for (state <- states) yield (state.text)

This returns another array buffer consisting of String objects. Next, we create a string variable which contains the base of the URL. We then concatenate each object from our stateNames array to our census_url string in order to create an array called zip_links containing the URL to all 53 states/territories ZIP files as follows:

census_url: String = "https://www2.census.gov/programs-surveys/decennial/2020/data/01-Redistricting_File--PL_94-171/"
val zip_links = for (link <- state_links2) yield Jsoup.connect(link).get()

TO summarize where we're at so far: we got the HTML tags of all of our websites, we removed the tags and extracted pure string values, and then appended those strings onto the end of the FTP server URL to get an array contaning URLs for every state and territory. We now need to repeat this process again, except we're doing it for all 53 states. This is accomplished in much the same way, so I will lay out the code in bulk and explain further.

val mytags2 = for (hyperlink <- zip_links) yield hyperlink.select("tbody tr:eq(3) td a")
val testing = for (x <- mytags2) yield (x.text)
zipfiles = for (i <- Range(0,52)) yield state_links2(i).concat(testing(i))

In the first part of our code, we only had 1 URL to modify in 53 separate ways. In this next part, we have 53 unique url's to modify. Thus we make extensive use of for-comprehensions. (Authors note - Scala truly is a great language for this kind of work! I'd web scrape in Scala over Python any day of the week! -JY) The first line of code creates an array buffer of string values containing the base URLs for all 53 state/territories. Then the second value testing yields an array buffer containing the portions of the URL for each state/territory that will be appended onto their respective base URLs. Finally, zipfiles returns us another array buffer where we append all the values from testing onto all of the values from mytags2 to get a complete URL that leads to the ZIP file containing our payload (spreadsheets with census data)! The only remaining task is to actually download the ZIP files.

In order to accomplish this, I borrowed a chunk of code from Rizwan Kaiser (https://medium.com/@rqaiserr/download-files-from-the-web-using-scala-a20afe0b2446):

def downloadFiles(downloadableFileLinks: List[String], path: String): Unit = {
  downloadableFileLinks.map( downloadableLink => {
  val urlObject = new URL(downloadableLink)
  val filePath = path + "/" + urlObject.getPath().replaceAll("/", "")
  urlObject #> new File(filePath)!!
  })
  }
  
Kaiser explains his function as follows: 
  "given a set of links that are downloadable, and a target path, will download the files. Below, for each downloadable link, we'll instantiate a new URL object, then we will create our target filepath with the filename. Then, we will clean the file name. Finally, we will write to disk. Map functions are super cool! they essentially apply logic to each element in your collection. Below, for each downloadableLink in downloadableFileLinks, we're initializing a URL, creating a filepath, and then writing the urlObject to disk."
  
Lastly, we turn our zipfiles array into a list and then run the function:
val zips = zipfiles.toList
downloadFiles(zips, "filepath to destination you want on your local machine for the files")

Voila! Rinse and repeat for 2010 and 2000 :)
